{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AucNnMCaSEP3"
      },
      "outputs": [],
      "source": [
        "## Library bb_eval_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M5Mv3VHSEP4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.path as pltPath\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "''' \n",
        "    target_idx indicate whether we are working on binary seg or multi seg problem. \n",
        "    this idx will be used to retrieve correct mask from ground truth, where:\n",
        "    0 - background\n",
        "    1 - skin\n",
        "    2 - eczema\n",
        "'''\n",
        "def compute_coverage_precision(true_mask, boxes, target_idx=2):\n",
        "\n",
        "    pixel_box_num = np.zeros(shape=true_mask.shape, dtype=np.int8)\n",
        "    # print('before', boxes)\n",
        "\n",
        "    boxes_area = 0\n",
        "    pixels_in_box_and_mask = 0\n",
        "    pixels_in_mask = 0\n",
        "\n",
        "    # coordinate transformation\n",
        "    for box in boxes:\n",
        "        for point in box:\n",
        "            point[1] = true_mask.shape[0] - point[1]\n",
        "\n",
        "    # iterate over all pixels to find if it is contained in box(es)\n",
        "    for i in range(true_mask.shape[0]):\n",
        "        for j in range(true_mask.shape[1]):\n",
        "            # count the number of eczema (or skin) pixels in mask (E_m)\n",
        "            if true_mask[i, j, target_idx] > 0:\n",
        "                pixels_in_mask += 1\n",
        "\n",
        "            for box in boxes:\n",
        "                path = pltPath.Path([box[0], box[1], box[2], box[3]])\n",
        "                if path.contains_points([[j,true_mask.shape[0] - i]]):\n",
        "                    # count pixels in both ground truth mask and boxes (TP)\n",
        "                    if true_mask[i, j, target_idx] > 0:\n",
        "                        pixels_in_box_and_mask += 1\n",
        "                        # debug\n",
        "                        \n",
        "\n",
        "                    pixel_box_num[i,j] += 100\n",
        "                    # accumulate to calculate the union region of boxes (A_b)\n",
        "                    boxes_area += 1\n",
        "                    break\n",
        "\n",
        "    plt.imshow(pixel_box_num)\n",
        "    cv2.imwrite(\"/path_to_proj_dir/output/predictions/test/test.jpg\", pixel_box_num)\n",
        "\n",
        "    # area_all_crops = true_mask.shape[0] * true_mask.shape[1]\n",
        "    # area = true_mask.shape[0] * true_mask.shape[1]\n",
        "\n",
        "    # define evaluation metrics\n",
        "    coverage = 0 if pixels_in_mask==0 else pixels_in_box_and_mask / pixels_in_mask\n",
        "    precision = 0 if boxes_area==0 else pixels_in_box_and_mask / boxes_area\n",
        "\n",
        "    # print(pixels_in_mask / area)\n",
        "    print('current coverage=', coverage)\n",
        "    print('current precision=', precision)\n",
        "\n",
        "    return coverage, precision\n",
        "\n",
        "def compute_f1(cov, prec):\n",
        "    #f1 = 2* c*q / (c+q)\n",
        "    # print(\"### Standard error of the f1 score ###\")\n",
        "    se_prec = np.std(prec)/math.sqrt(len(prec)) #se = standard error\n",
        "    se_cov = np.std(cov)/math.sqrt(len(cov))\n",
        "    se_prec_cov = se_prec + se_cov\n",
        "\n",
        "    #re = relative_error\n",
        "    re_f1 = se_prec/np.mean(prec) + se_cov/np.mean(cov) + se_prec_cov/(np.mean(prec)+np.mean(cov))\n",
        "    f1 =  2*np.mean(prec)*np.mean(cov)/(np.mean(prec)+np.mean(cov))\n",
        "\n",
        "    # print(\"F1 score:\", 2*np.mean(prec)*np.mean(cov)/(np.mean(prec)+np.mean(cov)), \"+/-\", re_f1)\n",
        "    return f1, re_f1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-R1_P20SEP5"
      },
      "outputs": [],
      "source": [
        "# @Author:  zihaowang\n",
        "# @Email:   zihao.wang20@alumni.imperial.ac.uk\n",
        "# @Website: www.wangzihao.org\n",
        "# @Date:    2021-01-19 15:29:02\n",
        "# @Last Modified by:   zihaowang\n",
        "# @Last Modified time: 2021-02-08 23:29:41\n",
        "\n",
        "\"\"\"# Dataloader and utility functions \"\"\"\n",
        "# helper function for data visualization\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import cv2\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "\n",
        "def visualize(**images):\n",
        "    \"\"\" Plot images in one row \"\"\"\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "# helper function for data visualization    \n",
        "def denormalize(x):\n",
        "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
        "    x_max = np.percentile(x, 98)\n",
        "    x_min = np.percentile(x, 2)    \n",
        "    x = (x - x_min) / (x_max - x_min)\n",
        "    x = x.clip(0, 1)\n",
        "    return x\n",
        "\n",
        "def run_sigle_pred(model, input_dir, output_dir, file_name, preprocessing=None, target_idx=2, resize_ratio=0.4, refno=None, visno=None):\n",
        "    # config path\n",
        "    mask_dir = os.path.join(output_dir, \"masks\")\n",
        "    crop_dir = os.path.join(output_dir, \"crops\")\n",
        "    box_dir = os.path.join(output_dir, \"boxes\")\n",
        "    \n",
        "    \n",
        "    # create folder if not exist\n",
        "    if not os.path.exists(mask_dir):\n",
        "        os.system(\"mkdir -p \" + mask_dir)\n",
        "    if not os.path.exists(crop_dir):\n",
        "        os.system(\"mkdir -p \" + crop_dir)\n",
        "    if not os.path.exists(box_dir):\n",
        "        os.system(\"mkdir -p \" + box_dir)\n",
        "    print(\"directory created!\")\n",
        "    \n",
        "    # clear previous output for same image\n",
        "    print(\"checking previous duplicate croppings...\")\n",
        "    cmd = \"rm \" + crop_dir + \"/\" + file_name.split(\".\")[0] + \"*.jpg\"\n",
        "    os.system(cmd)\n",
        "    \n",
        "    \n",
        "    # read and resize image\n",
        "    image = cv2.imread(input_dir)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    image_height, image_width, channels = image.shape\n",
        "    target_height = int((image_height / 32) * resize_ratio) * 32\n",
        "    target_width = int((image_width / 32) * resize_ratio) * 32\n",
        "    \n",
        "    image = cv2.resize(image, (target_width, target_height), interpolation = cv2.INTER_AREA)\n",
        "    image_cp = image.copy()\n",
        "    \n",
        "    # preprocessing (lambda transformation) for prediction\n",
        "    if preprocessing:\n",
        "        sample = preprocessing(image=image)\n",
        "        image = sample['image']\n",
        "        \n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    \n",
        "    # get prediction\n",
        "    pr_mask = model.predict(image)\n",
        "\n",
        "    # change the last number to decide which mask to output. [0: background; 1: skin; 2: eczema]\n",
        "    pr_img = pr_mask[0,:,:,target_idx]\n",
        "    pr_img = (pr_img * 255).astype(np.uint8)\n",
        "    \n",
        "    # output predicted mask\n",
        "    cv2.imwrite(os.path.join(mask_dir, \"pred_\" + file_name), pr_img)\n",
        "    # debug\n",
        "    # plt.figure()\n",
        "    # plt.imshow(pr_img)\n",
        "    # print(\"mask saved!\")\n",
        "    \n",
        "    # generate bounding boxes for each predicted mask\n",
        "    mask = cv2.imread(os.path.join(mask_dir, \"pred_\" + file_name))\n",
        "    result = mask.copy()\n",
        "    gray = cv2.cvtColor(mask,cv2.COLOR_BGR2GRAY)\n",
        "    thresh = cv2.threshold(gray,127,255,cv2.THRESH_BINARY)[1]\n",
        "    contours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contours = contours[0] if len(contours) == 2 else contours[1]\n",
        "    num_crops = 0\n",
        "    crop_fns = []\n",
        "    \n",
        "#     image_cp = cv2.bitwise_and(image_cp, image_cp, mask = thresh)\n",
        "    # debug crops\n",
        "#     plt.figure()\n",
        "#     plt.imshow(image_cp)\n",
        "    \n",
        "    # iterate through boxes\n",
        "    for cntr in contours:\n",
        "        rect = cv2.minAreaRect(cntr)\n",
        "        box = cv2.boxPoints(rect)\n",
        "        box = np.int0(box)\n",
        "        area = cv2.contourArea(cntr)\n",
        "        \n",
        "        # drop boxes with small area\n",
        "        if(area > 2500.0):\n",
        "            img_crop = crop_rect(image_cp, rect, box)\n",
        "            crop_file_name = refno + \"_vis-\" + visno + \"_\" + file_name.split(\".\")[0] + \"_crop-\" + str(num_crops) + \".jpg\"\n",
        "            plt.imsave(os.path.join(crop_dir, crop_file_name), img_crop)\n",
        "            crop_fns.append(crop_file_name)\n",
        "            result = cv2.drawContours(result,[box],0,(0,0,255),2)\n",
        "            num_crops += 1\n",
        "            \n",
        "            \n",
        "    # save bounding boxes\n",
        "    box_file_name = refno + \"_vis-\" + visno + \"_\" + file_name.split(\".\")[0] + \"_UNet.jpg\"\n",
        "    cv2.imwrite(os.path.join(box_dir, box_file_name), result) \n",
        "    \n",
        "    print(file_name, \": cropping done!\")\n",
        "    \n",
        "    return num_crops, crop_fns\n",
        "\n",
        "\n",
        "def crop_rect(image, rect, box):    \n",
        "    width = int(rect[1][0])\n",
        "    height = int(rect[1][1])\n",
        "    \n",
        "    src_pts = box.astype(\"float32\")\n",
        "    # coordinate of the points in box points after the rectangle has been\n",
        "    # straightened\n",
        "    dst_pts = np.array([[0, height-1],\n",
        "                        [0, 0],\n",
        "                        [width-1, 0],\n",
        "                        [width-1, height-1]], dtype=\"float32\")\n",
        "\n",
        "    # the perspective transformation matrix\n",
        "    M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
        "\n",
        "    # directly warp the rotated rectangle to get the straightened rectangle\n",
        "    warped = cv2.warpPerspective(image, M, (width, height))\n",
        "    \n",
        "    return warped\n",
        "\n",
        "# classes for data loading and preprocessing\n",
        "class Dataset:\n",
        "    \"\"\" Read images, apply augmentation and preprocessing transformations.\n",
        "\n",
        "    Args:\n",
        "        images_dir (str): path to images folder\n",
        "        masks_dir (str): path to segmentation masks folder\n",
        "        class_values (list): values of classes to extract from segmentation mask\n",
        "        augmentation (albumentations.Compose): data transfromation pipeline \n",
        "            (e.g. flip, scale, etc.)\n",
        "        preprocessing (albumentations.Compose): data preprocessing \n",
        "            (e.g. noralization, shape manipulation, etc.)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    CLASSES = ['background', 'skin', 'eczema']\n",
        "    # CLASSES = ['background', 'skin']\n",
        "    def __init__(\n",
        "            self, \n",
        "            images_dir, \n",
        "            masks_dir, \n",
        "            classes=None, \n",
        "            augmentation=None, \n",
        "            preprocessing=None,\n",
        "            is_train=True,\n",
        "            use_full_resolution=False,\n",
        "            binary_seg=False,\n",
        "    ):\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
        "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
        "\n",
        "        # convert str names to class values on masks\n",
        "        # self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
        "        # self.class_values = [0, 127, 255]\n",
        "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
        "\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "        self.use_full_resolution = use_full_resolution\n",
        "        self.is_train = is_train\n",
        "        self.binary_seg = binary_seg\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        # read data\n",
        "        image = cv2.imread(self.images_fps[i])\n",
        "        # convert BGR to RGB\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # define whether or not to resize the input stream to save program running memory\n",
        "        # note: the size of input has to be divisible by 32 for tensorflow to process\n",
        "        if not self.use_full_resolution:\n",
        "            target_ratio = 0.2\n",
        "        else:\n",
        "            target_ratio = 1\n",
        "\n",
        "        image_height, image_width, channels = image.shape\n",
        "        target_height = 768 if self.is_train else int((image_height / 32) * target_ratio) * 32\n",
        "        target_width = 1024 if self.is_train else int((image_width / 32) * target_ratio) * 32\n",
        "        image = cv2.resize(image, (target_width, target_height), interpolation = cv2.INTER_AREA)\n",
        "\n",
        "        # load the mask in gray scale, RGB -> single value\n",
        "        mask = cv2.imread(self.masks_fps[i], 0)\n",
        "        # 21 Jan, 2021: resize the mask to save memory\n",
        "        mask = cv2.resize(mask, (target_width, target_height), interpolation = cv2.INTER_AREA)\n",
        "        # reorganize the RGB value in masks\n",
        "\n",
        "        if self.binary_seg:\n",
        "            # binary segmentation\n",
        "            mask[mask > 1] = 1\n",
        "        else:\n",
        "            # multi-class classfication\n",
        "            mask[mask == 127] = 1\n",
        "            mask[mask == 255] = 2\n",
        "            mask[mask > 2] = 1\n",
        "\n",
        "        # print(np.unique(mask))\n",
        "        \n",
        "        # extract certain classes from mask (e.g. cars)\n",
        "        masks = [(mask == v) for v in self.class_values]\n",
        "        mask = np.stack(masks, axis=-1).astype('float')\n",
        "        # debug msg\n",
        "        # print(mask.shape)\n",
        "\n",
        "        # # apply augmentations\n",
        "        # if self.augmentation:\n",
        "        #     sample = self.augmentation(image=image, mask=mask)\n",
        "        #     image, mask = sample['image'], sample['mask']\n",
        "\n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "\n",
        "class Dataloder(keras.utils.Sequence):\n",
        "    \"\"\"Load data from dataset and form batches\n",
        "\n",
        "    Args:\n",
        "        dataset: instance of Dataset class for image loading and preprocessing.\n",
        "        batch_size: Integet number of images in batch.\n",
        "        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(dataset))\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        # collect batch data\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        "\n",
        "        # transpose list of lists\n",
        "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
        "\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
        "        return len(self.indexes) // self.batch_size\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n",
        "        if self.shuffle:\n",
        "            self.indexes = np.random.permutation(self.indexes)\n",
        "\n",
        "\n",
        "def round_clip_0_1(x, **kwargs):\n",
        "    return x.round().clip(0, 1)\n",
        "\n",
        "# define heavy augmentations\n",
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "\n",
        "        A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
        "\n",
        "        A.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
        "        A.RandomCrop(height=320, width=320, always_apply=True),\n",
        "\n",
        "        A.IAAAdditiveGaussianNoise(p=0.2),\n",
        "        A.IAAPerspective(p=0.5),\n",
        "\n",
        "        A.OneOf(\n",
        "            [\n",
        "                A.CLAHE(p=1),\n",
        "                A.RandomBrightness(p=1),\n",
        "                A.RandomGamma(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "\n",
        "        A.OneOf(\n",
        "            [\n",
        "                A.IAASharpen(p=1),\n",
        "                A.Blur(blur_limit=3, p=1),\n",
        "                A.MotionBlur(blur_limit=3, p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "\n",
        "        A.OneOf(\n",
        "            [\n",
        "                A.RandomContrast(p=1),\n",
        "                A.HueSaturationValue(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "        A.Lambda(mask=round_clip_0_1)\n",
        "    ]\n",
        "    return A.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
        "    test_transform = [\n",
        "        A.PadIfNeeded(384, 480)\n",
        "    ]\n",
        "    return A.Compose(test_transform)\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    \"\"\"Construct preprocessing transform\n",
        "\n",
        "    Args:\n",
        "        preprocessing_fn (callbale): data normalization function \n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _transform = [\n",
        "        A.Lambda(image=preprocessing_fn),\n",
        "    ]\n",
        "    return A.Compose(_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlDMBSMGSEP8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import cv2\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models as sm\n",
        "sm.set_framework('keras')\n",
        "import csv\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "################################ Model loading (this part will be replaced with new data type soon) ################################\n",
        "BIN_SEG = True\n",
        "\n",
        "MODEL_NAME = '/mul_seg_model.h5'\n",
        "CLASSES = ['background', 'skin', 'eczema']\n",
        "WEIGHTS = np.array([1, 1, 1])\n",
        "target_idx = 2\n",
        "\n",
        "if BIN_SEG:\n",
        "    MODEL_NAME = '/bin_seg_model.h5'\n",
        "    CLASSES = ['background', 'skin']\n",
        "    WEIGHTS = np.array([1, 1])\n",
        "    target_idx = 1\n",
        "\n",
        "BACKBONE = 'efficientnetb3'\n",
        "LR = 0.0001\n",
        "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
        "\n",
        "\"\"\"# Model Evaluation\"\"\"\n",
        "# config PROJ_DIR according to your environment\n",
        "PROJ_DIR = \"/path_to_proj_dir\"\n",
        "PRED_DIR = os.path.join(PROJ_DIR, 'output/predictions/test')\n",
        "BB_DIR = os.path.join(PROJ_DIR, 'output/bounding_boxes/test')\n",
        "EVAL_DIR = os.path.join(PROJ_DIR, 'output/evaluations')\n",
        "MODEL_DIR = os.path.join(PROJ_DIR, 'output')\n",
        "DATA_DIR = os.path.join(PROJ_DIR, 'data')\n",
        "\n",
        "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
        "y_test_dir = os.path.join(DATA_DIR, 'testannot')\n",
        "\n",
        "test_dataset = Dataset(\n",
        "    x_test_dir, \n",
        "    y_test_dir, \n",
        "    classes=CLASSES, \n",
        "    augmentation=None,\n",
        "    preprocessing=get_preprocessing(preprocess_input),\n",
        "    is_train=False,\n",
        "    use_full_resolution=False,\n",
        "    binary_seg=BIN_SEG,\n",
        ")\n",
        "eczema_dataset = Dataset(\n",
        "    x_test_dir, \n",
        "    y_test_dir, \n",
        "    classes=['background', 'skin', 'eczema'], \n",
        "    augmentation=None,\n",
        "    preprocessing=get_preprocessing(preprocess_input),\n",
        "    is_train=False,\n",
        "    use_full_resolution=False,\n",
        "    binary_seg=0,\n",
        ")\n",
        "test_dataloader = Dataloder(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# define network parameters\n",
        "n_classes = len(CLASSES)\n",
        "# select training mode\n",
        "activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
        "#create model\n",
        "model = sm.Unet(BACKBONE, classes=n_classes, activation=activation)\n",
        "# define optomizer\n",
        "optim = keras.optimizers.Adam(LR)\n",
        "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
        "# Set class weights for diss_loss (background: 1, skin: 1, eczema: 1)\n",
        "dice_loss = sm.losses.DiceLoss(class_weights=WEIGHTS)\n",
        "focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n",
        "total_loss = dice_loss + (1 * focal_loss)\n",
        "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
        "# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
        "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model.compile(optim, total_loss, metrics)\n",
        "\n",
        "# load trained segmentation model\n",
        "model.load_weights(MODEL_DIR + MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmcaMeHBSEP8",
        "outputId": "e3a7d31d-0a07-4fb3-cfb9-e9e3560f492c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clearing previous masks...\n",
            "Done! Now saving new prediction masks...\n",
            "current coverage= 0.7911027666953506\n",
            "current precision= 0.2544173759906076\n",
            "current coverage= 0.9948234933762157\n",
            "current precision= 0.4333875982439519\n",
            "Done!\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "################################ Mask prediction and evaluation ################################\n",
        "\"\"\"# Saving Masks Predictions\"\"\"\n",
        "# save all predictions \n",
        "# clear previous predictions\n",
        "print('Clearing previous masks...')\n",
        "os.system(\"rm \" + PRED_DIR + \"/*.jpg\")\n",
        "os.system(\"rm \" + PRED_DIR + \"/*.JPG\")\n",
        "os.system(\"rm \" + BB_DIR + \"/*.jpg\")\n",
        "os.system(\"rm \" + BB_DIR + \"/*.JPG\")\n",
        "os.system(\"rm \" + EVAL_DIR + \"/test.csv\")\n",
        "print('Done! Now saving new prediction masks...')\n",
        "\n",
        "# Feb 8: export evaluation result as csv file \n",
        "cov = []\n",
        "prec = []\n",
        "with open(EVAL_DIR + '/test.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"file_name\", \"coverage\", \"precision\", \"f1_score\", \"standard_error\"])\n",
        "#     for i in range(len(test_dataset)):\n",
        "    for i in range(2):\n",
        "        # save predicted masks\n",
        "        image, _ = test_dataset[i]\n",
        "        _, gt_mask = eczema_dataset[i]\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        pr_mask = model.predict(image)\n",
        "        # change the last number to decide which mask to output. [0: background; 1: skin; 2: eczema]\n",
        "        pr_img = pr_mask[0,:,:,target_idx]\n",
        "        pr_img = (pr_img * 255).astype(np.uint8)\n",
        "        cv2.imwrite(os.path.join(PRED_DIR, \"pred_\" + test_dataset.ids[i]), pr_img)\n",
        "        # generate bounding boxes for each predicted mask\n",
        "        boxes = []\n",
        "        img = cv2.imread(os.path.join(PRED_DIR, \"pred_\" + test_dataset.ids[i]))\n",
        "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "        thresh = cv2.threshold(gray,127,255,cv2.THRESH_BINARY)[1]\n",
        "        result = img.copy()\n",
        "        contours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        contours = contours[0] if len(contours) == 2 else contours[1]\n",
        "        for cntr in contours:\n",
        "            rect = cv2.minAreaRect(cntr)\n",
        "            box = cv2.boxPoints(rect)\n",
        "            box = np.int0(box)\n",
        "            area = cv2.contourArea(cntr)\n",
        "            # Abandon boxes with too small area\n",
        "            if(area > 2500):\n",
        "                boxes.append(box)\n",
        "                result = cv2.drawContours(result,[box],0,(0,0,255),2)\n",
        "\n",
        "\n",
        "        # Feb 8: compute performance of bounding boxes\n",
        "        coverage_per_image, precision_per_image = compute_coverage_precision(gt_mask, boxes, target_idx=2)\n",
        "        cov.append(coverage_per_image)\n",
        "        prec.append(precision_per_image)\n",
        "        writer.writerow([test_dataset.ids[i], coverage_per_image, precision_per_image])\n",
        "        # save bounding boxes\n",
        "        cv2.imwrite(os.path.join(BB_DIR, \"bb_\" + test_dataset.ids[i]), result)    \n",
        "    # Feb 8: append the mean performance to the end of csv\n",
        "    f1, se = compute_f1(cov, prec)\n",
        "    writer.writerow(['overall', np.mean(cov), np.mean(prec), f1, se])\n",
        "    print('Done!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8SDjqsgSEP9"
      },
      "outputs": [],
      "source": [
        "!python ../src/eval_of_ad_identification.py --seg_type ad --suffix test --model_dir /ad_base_ce_bestOnly/base_ad_ce.h5"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:SEG_DL]",
      "language": "python",
      "name": "conda-env-SEG_DL-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}